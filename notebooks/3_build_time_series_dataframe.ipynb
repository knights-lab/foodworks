{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaborn 0.11.1\n",
      "numpy   1.19.5\n",
      "pandas  1.1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(930525)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "warnings.simplefilter('once')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/type_1/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warp(a, b, s):\n",
    "\treturn (s - b) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the alignment\n",
    "df_alignment = pd.read_csv(\"../results/alignment_results.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "user_names = list(df_alignment[\"reference\"].unique())\n",
    "\n",
    "user_names_train = random.sample(user_names, k=round(len(user_names) * 0.8))\n",
    "\n",
    "\n",
    "mask_reference = np.array([name in user_names_train for name in df_alignment[\"reference\"]])\n",
    "mask_current = np.array([name in user_names_train for name in df_alignment[\"current\"]])\n",
    "\n",
    "# df_alignment_train_error = df_alignment.copy().loc[mask_reference & mask_current]\n",
    "df_alignment_train_error = df_alignment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-49cff709e01f>:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  df_alignment_error.values[[np.arange(df_alignment_error.shape[0])]*2] = 0\n"
     ]
    }
   ],
   "source": [
    "# get the best person to align to\n",
    "df_alignment_error = df_alignment_train_error.pivot(index=\"reference\", columns=\"current\", values=\"error\").copy()\n",
    "df_alignment_error.values[[np.arange(df_alignment_error.shape[0])]*2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_alignment_error.copy().loc[~mask_train]\n",
    "\n",
    "best_aligner = df_alignment_error.mean(axis=1).sort_values().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MCTs30'], dtype='object', name='reference')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_users = df_alignment_error.index[df_alignment_error[best_aligner] > df_alignment_error[best_aligner].mean() + (2*df_alignment_error[best_aligner].std())]\n",
    "drop_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/type_1/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# load up the data.frame\n",
    "df_tax_counts = pd.read_csv(\"../data/taxonomy_clr_s_top.txt\", index_col=0, sep=\"\\t\")\n",
    "\n",
    "# load up the splines\n",
    "import pickle\n",
    "with open(\"../results/d_splines.pkl\", \"rb\") as inf:\n",
    "    d_splines = pickle.load(inf)\n",
    "\n",
    "# load up the spline dataframe\n",
    "df_tax_splines = pd.read_csv(\"../results/tax_clr_splines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "PRESENCE_THRESHOLD = .95\n",
    "SAMPLING_RATE = 1\n",
    "OVERLAP_THRESHOLD = .5\n",
    "\n",
    "max_study_day_no = df_tax_splines[\"StudyDayNo\"].max()\n",
    "min_study_day_no = df_tax_splines[\"StudyDayNo\"].min()\n",
    "\n",
    "index_splines = np.arange(min_study_day_no, max_study_day_no + 1, SAMPLING_RATE, dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy\n",
    "dfs = []\n",
    "for (current, taxonomy), df in df_tax_splines.groupby([\"UserName\", \"feature\"]):\n",
    "    spline_current, current_min, current_max = d_splines[current][taxonomy]\n",
    "    row = df_alignment.query(f\"reference == '{best_aligner}' & current == '{current}'\")\n",
    "    index_warp = linear_warp(row['a'].values[0], row['b'].values[0], index_splines)\n",
    "\n",
    "    ts_current_spline = interpolate.splev(index_warp, spline_current)\n",
    "\n",
    "    ts_current_spline = np.clip(ts_current_spline, current_min, current_max)\n",
    "    \n",
    "    study_day_num = df.loc[np.isfinite(df[\"spline\"]), \"StudyDayNo\"]\n",
    "    index_min = np.min(study_day_num)\n",
    "    index_max = np.max(study_day_num)\n",
    "    ts_current_spline[index_warp < index_min] = np.nan\n",
    "    ts_current_spline[index_warp > index_max] = np.nan\n",
    "    \n",
    "    df[\"temporal_warp_spline\"] = ts_current_spline\n",
    "    \n",
    "    df[\"index_warp\"] = index_warp\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "df_temporal_warp = pd.concat(dfs)\n",
    "\n",
    "df_temporal_warp_wide_train = df_temporal_warp.query(\"index_warp <= 12\").pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"temporal_warp_spline\")\n",
    "df_temporal_warp_wide_train[\"train\"] = True\n",
    "df_temporal_warp_wide_test = df_temporal_warp.query(\"index_warp > 12\").pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"temporal_warp_spline\")\n",
    "df_temporal_warp_wide_test[\"train\"] = False\n",
    "\n",
    "df_temporal_warp_wide = pd.concat([df_temporal_warp_wide_train, df_temporal_warp_wide_test])\n",
    "\n",
    "df_tax_wide = df_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"spline\")\n",
    "\n",
    "df_temporal_warp_wide.columns = [\"tax;\" + column for column in df_temporal_warp_wide.columns]\n",
    "df_tax_wide.columns = [\"tax;\" + column for column in df_tax_wide.columns]\n",
    "df_temporal_warp_wide = df_temporal_warp_wide.rename({\"tax;train\": \"train\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/type_1/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# load up the splines\n",
    "with open(\"../results/d_kegg_splines.pkl\", \"rb\") as inf:\n",
    "    d_splines = pickle.load(inf)\n",
    "\n",
    "# load up the spline dataframe\n",
    "df_kegg_splines = pd.read_csv(\"../results/kegg_clr_splines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegg\n",
    "dfs = []\n",
    "for (current, taxonomy), df in df_kegg_splines.groupby([\"UserName\", \"feature\"]):\n",
    "    spline_current, current_min, current_max = d_splines[current][taxonomy]\n",
    "    row = df_alignment.query(f\"reference == '{best_aligner}' & current == '{current}'\")\n",
    "    index_warp = linear_warp(row['a'].values[0], row['b'].values[0], index_splines)\n",
    "\n",
    "    ts_current_spline = interpolate.splev(index_warp, spline_current)\n",
    "\n",
    "    ts_current_spline = np.clip(ts_current_spline, current_min, current_max)\n",
    "    \n",
    "    study_day_num = df.loc[np.isfinite(df[\"spline\"]), \"StudyDayNo\"]\n",
    "    index_min = np.min(study_day_num)\n",
    "    index_max = np.max(study_day_num)\n",
    "    ts_current_spline[index_warp < index_min] = np.nan\n",
    "    ts_current_spline[index_warp > index_max] = np.nan\n",
    "       \n",
    "    df[\"temporal_warp_spline\"] = ts_current_spline\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df_kegg_temporal_warp = pd.concat(dfs)\n",
    "df_kegg_temporal_warp_wide = df_kegg_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"temporal_warp_spline\")\n",
    "df_kegg_wide = df_kegg_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"spline\")\n",
    "\n",
    "df_kegg_temporal_warp_wide.columns = [\"kegg;\" + column for column in df_kegg_temporal_warp_wide.columns]\n",
    "df_kegg_wide.columns = [\"kegg;\" + column for column in df_kegg_wide.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the splines\n",
    "with open(\"../results/d_nutrient_splines.pkl\", \"rb\") as inf:\n",
    "    d_splines = pickle.load(inf)\n",
    "\n",
    "# load up the spline dataframe\n",
    "df_nutrients_splines = pd.read_csv(\"../results/nutrients_splines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy\n",
    "dfs = []\n",
    "for (current, taxonomy), df in df_nutrients_splines.groupby([\"UserName\", \"feature\"]):\n",
    "    spline_current, current_min, current_max = d_splines[current][taxonomy]\n",
    "    row = df_alignment.query(f\"reference == '{best_aligner}' & current == '{current}'\")\n",
    "    index_warp = linear_warp(row['a'].values[0], row['b'].values[0], index_splines)\n",
    "\n",
    "    ts_current_spline = interpolate.splev(index_warp, spline_current)\n",
    "\n",
    "    ts_current_spline = np.clip(ts_current_spline, current_min, current_max)\n",
    "    \n",
    "    study_day_num = df.loc[np.isfinite(df[\"spline\"]), \"StudyDayNo\"]\n",
    "    index_min = np.min(study_day_num)\n",
    "    index_max = np.max(study_day_num)\n",
    "    ts_current_spline[index_warp < index_min] = np.nan\n",
    "    ts_current_spline[index_warp > index_max] = np.nan\n",
    "       \n",
    "    df[\"temporal_warp_spline\"] = ts_current_spline\n",
    "    dfs.append(df)\n",
    "\n",
    "df_nutrients_temporal_warp = pd.concat(dfs)\n",
    "df_nutrients_temporal_warp_wide = df_nutrients_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"temporal_warp_spline\")\n",
    "df_nutrients_wide = df_nutrients_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"spline\")\n",
    "\n",
    "\n",
    "df_nutrients_temporal_warp_wide.columns = [\"nutrients;\" + column for column in df_nutrients_temporal_warp_wide.columns]\n",
    "df_nutrients_wide.columns = [\"nutrients;\" + column for column in df_nutrients_wide.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the splines\n",
    "with open(\"../results/d_food_splines.pkl\", \"rb\") as inf:\n",
    "    d_splines = pickle.load(inf)\n",
    "\n",
    "# load up the spline dataframe\n",
    "df_food_splines = pd.read_csv(\"../results/food_L3_clr_splines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy\n",
    "dfs = []\n",
    "for (current, taxonomy), df in df_food_splines.groupby([\"UserName\", \"feature\"]):\n",
    "    spline_current, current_min, current_max = d_splines[current][taxonomy]\n",
    "    row = df_alignment.query(f\"reference == '{best_aligner}' & current == '{current}'\")\n",
    "    index_warp = linear_warp(row['a'].values[0], row['b'].values[0], index_splines)\n",
    "\n",
    "    ts_current_spline = interpolate.splev(index_warp, spline_current)\n",
    "\n",
    "    ts_current_spline = np.clip(ts_current_spline, current_min, current_max)\n",
    "    \n",
    "    study_day_num = df.loc[np.isfinite(df[\"spline\"]), \"StudyDayNo\"]\n",
    "    index_min = np.min(study_day_num)\n",
    "    index_max = np.max(study_day_num)\n",
    "    ts_current_spline[index_warp < index_min] = np.nan\n",
    "    ts_current_spline[index_warp > index_max] = np.nan\n",
    "       \n",
    "    df[\"temporal_warp_spline\"] = ts_current_spline\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df_food_temporal_warp = pd.concat(dfs)\n",
    "df_food_temporal_warp_wide = df_food_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"temporal_warp_spline\")\n",
    "df_food_wide = df_food_temporal_warp.pivot(index=[\"UserName\", \"StudyDayNo\"], columns=\"feature\", values=\"spline\")\n",
    "\n",
    "\n",
    "df_food_temporal_warp_wide.columns = [\"food;\" + column for column in df_food_temporal_warp_wide.columns]\n",
    "df_food_wide.columns = [\"food;\" + column for column in df_food_wide.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift by one day\n",
    "df_day_plus_1_warp = df_temporal_warp_wide.groupby(\"UserName\").shift(-1)\n",
    "\n",
    "df_day_plus_1_warp.columns = [\"day_plus_one;\" + column for column in df_day_plus_1_warp.columns]\n",
    "\n",
    "df_day_plus_1 = df_tax_wide.groupby(\"UserName\").shift(-1)\n",
    "df_day_plus_1.columns = [\"day_plus_one;\" + column for column in df_day_plus_1.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_warp_network = pd.concat([df_temporal_warp_wide, df_food_temporal_warp_wide, df_nutrients_temporal_warp_wide, df_kegg_temporal_warp_wide, df_day_plus_1_warp], axis=1)\n",
    "df_network = pd.concat([df_tax_wide, df_food_wide, df_nutrients_wide, df_kegg_wide, df_day_plus_1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.read_csv(\"../data/SampleID_map.txt\", sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_columns = ['UserName', 'StudyDayNo', 'Gender', 'Age', 'Weight',\n",
    "       'Height', 'BMI', 'Supplement',\n",
    "       'oilGrams.assigned', 'Timing',\n",
    "       'Activity.Factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_warp = pd.merge(df_warp_network, df_mapping[mapping_columns], how=\"left\", on=[\"UserName\", \"StudyDayNo\"])\n",
    "df_merged = pd.merge(df_network, df_mapping[mapping_columns], how=\"left\", on=[\"UserName\", \"StudyDayNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[~df_merged[\"UserName\"].isin(drop_users)]\n",
    "df_merged_warp = df_merged_warp[~df_merged_warp[\"UserName\"].isin(drop_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_complete_cases_warp = df_merged_warp.dropna()\n",
    "df_merged_complete_cases = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/type_1/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1      MCTs01\n",
       "2      MCTs01\n",
       "3      MCTs01\n",
       "4      MCTs01\n",
       "5      MCTs01\n",
       "        ...  \n",
       "588    MCTs37\n",
       "589    MCTs37\n",
       "590    MCTs37\n",
       "591    MCTs37\n",
       "592    MCTs37\n",
       "Name: UserName, Length: 476, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_complete_cases[\"UserName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_complete_cases_warp.columns = [col.replace(\" \", \"_\").replace(\";\", \".\").replace(\"-\", \"_\") for col in df_merged_complete_cases_warp.columns]\n",
    "df_merged_complete_cases.columns = [col.replace(\" \", \"_\").replace(\";\", \".\").replace(\"-\", \"_\") for col in df_merged_complete_cases.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_splines_train = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_warp = df_merged_complete_cases_warp[\"train\"].values\n",
    "mask = np.array([name in index_splines_train for name in df_merged_complete_cases[\"StudyDayNo\"]])\n",
    "\n",
    "df_train_warp = df_merged_complete_cases_warp.iloc[mask_warp]\n",
    "df_test_warp = df_merged_complete_cases_warp.iloc[~mask_warp]\n",
    "df_train = df_merged_complete_cases.iloc[mask]\n",
    "df_test = df_merged_complete_cases.iloc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_warp = df_train_warp[df_train.columns]\n",
    "df_test_warp = df_test_warp[df_test_warp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(['-'.join(_) for _ in zip(df_test[\"UserName\"].values.astype(str), df_test[\"StudyDayNo\"].values.astype(str))])\n",
    "s_warp = set(['-'.join(_) for _ in zip(df_test_warp[\"UserName\"].values.astype(str), df_test_warp[\"StudyDayNo\"].values.astype(str))])\n",
    "\n",
    "mask = ['-'.join(_) in s_warp for _ in zip(df_test[\"UserName\"].values.astype(str), df_test[\"StudyDayNo\"].values.astype(str))]\n",
    "mask_warp = ['-'.join(_) in s for _ in zip(df_test_warp[\"UserName\"].values.astype(str), df_test_warp[\"StudyDayNo\"].values.astype(str))]\n",
    "\n",
    "df_test = df_test.loc[mask]\n",
    "df_test_warp = df_test_warp.loc[mask_warp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 232)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 232)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_warp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 232)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"UserName\"])\n",
    "df_train_warp = df_train_warp.drop(columns=[\"UserName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_names = []\n",
    "nutrient_names = []\n",
    "day_plus_one_names = []\n",
    "tax_names = []\n",
    "kegg_names = []\n",
    "other_names = []\n",
    "\n",
    "for column in column_names:\n",
    "    if column.startswith(\"day_plus_one.\"):\n",
    "        day_plus_one_names.append(column)\n",
    "    elif column.startswith(\"tax.\"):\n",
    "        tax_names.append(column)\n",
    "    elif column.startswith(\"food.\"):\n",
    "        food_names.append(column)\n",
    "    elif column.startswith(\"nutrients.\"):\n",
    "        nutrient_names.append(column)\n",
    "    elif column.startswith(\"kegg.\"):\n",
    "        kegg_names.append(column)\n",
    "    else:\n",
    "        other_names.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "food_names_blacklist = list(product(food_names, food_names + tax_names + kegg_names + other_names))\n",
    "\n",
    "nutrient_names_blacklist = list(product(nutrient_names, food_names + nutrient_names + tax_names + kegg_names + other_names))\n",
    "\n",
    "day_plus_one_blacklist = list(product(day_plus_one_names, column_names))\n",
    "\n",
    "tax_names_blacklist = list(product(tax_names, food_names + nutrient_names + tax_names + other_names))\n",
    "\n",
    "kegg_names_blacklist = list(product(kegg_names, food_names + nutrient_names + tax_names + kegg_names + other_names))\n",
    "\n",
    "other_names_blacklist = list(product(other_names, food_names + nutrient_names + tax_names + kegg_names + other_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = food_names_blacklist + nutrient_names_blacklist + day_plus_one_blacklist + tax_names_blacklist + kegg_names_blacklist + other_names_blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blacklist = pd.DataFrame(blacklist, columns=[\"from\", \"to\"])\n",
    "df_blacklist[\"from\"] = df_blacklist[\"from\"].str.replace(\";\", \".\")\n",
    "df_blacklist[\"to\"] = df_blacklist[\"to\"].str.replace(\";\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blacklist.to_csv(\"../data/blacklist.txt\", index=True, sep=\"\\t\")\n",
    "\n",
    "df_train.to_csv(\"../data/train.txt\", index=True, sep=\"\\t\")\n",
    "df_train_warp.to_csv(\"../data/train.warp.txt\", index=True, sep=\"\\t\")\n",
    "df_test.to_csv(\"../data/test.txt\", index=True, sep=\"\\t\")\n",
    "df_test_warp.to_csv(\"../data/test.warp.txt\", index=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-type_1]",
   "language": "python",
   "name": "conda-env-.conda-type_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
